<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://eugenebang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://eugenebang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-01T04:32:04+00:00</updated><id>https://eugenebang.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">🚀 Arc’s Virtual Cell Challenge: Accelerating Drug Discovery with AI Models</title><link href="https://eugenebang.github.io/blog/2025/virtual-cell-ark-institue/" rel="alternate" type="text/html" title="🚀 Arc’s Virtual Cell Challenge: Accelerating Drug Discovery with AI Models"/><published>2025-06-29T16:40:16+00:00</published><updated>2025-06-29T16:40:16+00:00</updated><id>https://eugenebang.github.io/blog/2025/virtual-cell-ark-institue</id><content type="html" xml:base="https://eugenebang.github.io/blog/2025/virtual-cell-ark-institue/"><![CDATA[<p>🧬 What if we could predict how cells change state under different conditions, and discover drugs to shift “diseased” cells back to “healthy” ones? The Arc Institute is taking a major step toward this vision with the launch of the inaugural Virtual Cell Challenge, a public competition offering $100,000+ in prizes for the best machine learning models predicting cellular responses to genetic perturbations.</p> <h4 id="-why-is-this-hard">🧪 Why is this hard?</h4> <p>“Cells are living dynamic systems,” explains Yusuf Roohani, PhD, ML group lead at Arc. “They’re constantly in flux, messy, and experiment-dependent.” Virtual cell models must account for cell type, genetic background, and context while navigating technical noise and inconsistent reproducibility across datasets—challenges that have slowed progress and made benchmarking difficult.</p> <h4 id="-a-new-benchmark-for-the-community">🏆 A new benchmark for the community</h4> <p>Modeled after CASP (which transformed protein structure prediction and paved the way for AlphaFold), Arc’s challenge—described in a new Cell commentary led by Roohani—aims to align the community around standardized evaluation for virtual cells. Sponsored by Nvidia, 10x Genomics, and Ultima Genomics, the competition invites academia, biotech, and independent researchers to participate.</p> <p>🔬 For the competition, Arc generated a new single-cell transcriptomics dataset of 300,000 human embryonic stem cells with 300 genetic perturbations, rolled out in phases for fine-tuning, validation, and testing. Models will be evaluated on:</p> <p>1️⃣ Predicting differentially expressed genes<br/> 2️⃣ Discriminating perturbation effects<br/> 3️⃣ General error in expression count deviation</p> <h3 id="state-virtual-cell-model-from-arc-institute">STATE: virtual cell model from Arc Institute</h3> <p>Competitors will initially face off against Arc’s own STATE model, released last week for non-commercial use. Composed of:</p> <p>🔹 <strong>State Transition (ST) module:</strong> Uses data from 100M+ perturbed cells across 70 contexts, leveraging a bi-directional transformer to predict perturbation effects across cell collections.<br/> 🔹 <strong>State Embedding (SE) module:</strong> Trained on 167M human cells to learn robust gene expression variation, optimized for detecting biological perturbations while handling technical noise.</p> <p>The STATE model has shown over 50% improvement in perturbation effect discrimination and 2x accuracy in identifying differentially expressed genes compared to existing models.</p> <h4 id="-why-this-matters">🩺 Why this matters:</h4> <p>If successful, virtual cell models could transform drug discovery and precision medicine, enabling the design of interventions with fewer off-target effects and higher clinical success rates.</p> <p>For those interested, registration is open now, with final evaluations happening in late October and winners revealed in December. This could be the next frontier in biology x AI—and your team could help build it. 🌱</p>]]></content><author><name></name></author><category term="SOTA"/><category term="Competition"/><category term="virtualcell"/><summary type="html"><![CDATA[The Arc Institute is taking a major step toward this vision with the launch of the inaugural Virtual Cell Challenge, a public competition offering $100,000+ in prizes for the best machine learning models predicting cellular responses to genetic perturbations.]]></summary></entry><entry><title type="html">🧬 DeepMind releases AlphaGenome, a New AI Model for Decoding the Genome</title><link href="https://eugenebang.github.io/blog/2025/alphagenome/" rel="alternate" type="text/html" title="🧬 DeepMind releases AlphaGenome, a New AI Model for Decoding the Genome"/><published>2025-06-25T16:40:16+00:00</published><updated>2025-06-25T16:40:16+00:00</updated><id>https://eugenebang.github.io/blog/2025/alphagenome</id><content type="html" xml:base="https://eugenebang.github.io/blog/2025/alphagenome/"><![CDATA[<p>DeepMind has unveiled <a href="https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/">AlphaGenome</a>, a <strong>unifying DNA sequence model</strong> that advances regulatory variant-effect prediction and promises to illuminate the function of the human genome—now available via API for non-commercial research.</p> <hr/> <h4 id="-why-it-matters">🔍 Why it matters</h4> <p>The genome is our cellular instruction manual, but deciphering how its instructions are read—and how small DNA variations affect biology—remains one of science’s deepest challenges. Variants in non-coding regions, which make up 98% of the genome, play critical roles in orchestrating gene activity and disease susceptibility.</p> <p>AlphaGenome aims to address this, predicting how single variants or mutations impact a wide range of biological processes regulating genes with higher resolution than before.</p> <h4 id="️-how-alphagenome-works">⚙️ How AlphaGenome works</h4> <p>AlphaGenome can process long DNA sequences (up to 1 million base-pairs) and predict thousands of molecular properties, including:</p> <p>🔹 Gene start and end sites across tissues<br/> 🔹 RNA production levels<br/> 🔹 DNA accessibility, proximity, and protein binding sites</p> <p>It evaluates the effects of mutations by comparing predictions for mutated and unmutated sequences, providing a direct readout of potential functional consequences.</p> <p>The architecture combines convolutional layers (to detect local patterns), transformers (to model long-range dependencies), and final modality-specific layers, distributing training across interconnected TPUs for efficiency.</p> <p>AlphaGenome builds on DeepMind’s previous model, Enformer, and complements AlphaMissense, extending variant-effect prediction from protein-coding regions (2% of the genome) to the vast non-coding regions that shape gene regulation.</p> <h4 id="-why-this-is-exciting">🌱 Why this is exciting</h4> <p>It seems like integration of 3D and multi-scale genomic information is becoming essential for decoding gene regulation and variant effects. AlphaGenome is a step toward that vision, offering:</p> <p>✅ A scalable, open-access tool for the research community<br/> ✅ Potential insights into disease mechanisms<br/> ✅ Foundations for new therapeutic discoveries</p> <p>By making AlphaGenome available via API, DeepMind is inviting the community to explore and validate this model, accelerating our collective progress in interpreting the genome’s language.</p>]]></content><author><name></name></author><category term="SOTA"/><category term="foundationmodel"/><summary type="html"><![CDATA[DeepMind releases AlphaGenome, a unifying DNA sequence model that advances regulatory variant-effect prediction, promising to shed fresh light on how the genome orchestrates life.]]></summary></entry><entry><title type="html">Introducing TxGemma: Open models to improve therapeutics development - Google Developers Blog</title><link href="https://eugenebang.github.io/blog/2025/introducing-txgemma-open-models-to-improve-therapeutics-development-google-developers-blog/" rel="alternate" type="text/html" title="Introducing TxGemma: Open models to improve therapeutics development - Google Developers Blog"/><published>2025-03-25T00:00:00+00:00</published><updated>2025-03-25T00:00:00+00:00</updated><id>https://eugenebang.github.io/blog/2025/introducing-txgemma-open-models-to-improve-therapeutics-development--------------------------------------google-developers-blog</id><content type="html" xml:base="https://eugenebang.github.io/blog/2025/introducing-txgemma-open-models-to-improve-therapeutics-development-google-developers-blog/"><![CDATA[<p>Developing a new therapeutic is risky, notoriously slow, and can cost billions of dollars. 90% of drug candidates fail beyond phase 1 trials. Today, we’re excited to release TxGemma, a collection of open models designed to improve the efficiency of therapeutic development by leveraging the power of large language models.Building on Google DeepMind’s Gemma, a family of lightweight, state-of-the-art open models, TxGemma is specifically trained to understand and predict the properties of therapeutic entities throughout the entire discovery process, from identifying promising targets to helping predict clinical trial outcomes. This can potentially shorten the time from lab to bedside, and reduce the costs associated with traditional methods.Last October, we introduced Tx-LLM, a language model trained for a variety of therapeutic tasks related to drug development. After huge interest to use and fine-tune this model for therapeutic applications, we have developed its open successor at a practical scale: TxGemma, which we are releasing today for developers to adapt to their own therapeutic data and tasks.TxGemma models, fine-tuned from Gemma 2 using 7 million training examples, are open models designed for prediction and conversational therapeutic data analysis. These models are available in three sizes: 2B, 9B and 27B. Each size includes a ‘predict’ version, specifically tailored for narrow tasks drawn from Therapeutic Data Commons, for example predicting if a molecule is toxic.These tasks encompass:The largest TxGemma model (27B predict version) delivers strong performance. It’s not only better than, or roughly equal to, our previous state-of-the-art generalist model (Tx-LLM) on almost every task, but it also rivals or beats many models that are specifically designed for single tasks. Specifically, it outperforms or has comparable performance to our previous model on 64 of 66 tasks (beating it on 45), and does the same against specialized models on 50 of the tasks (beating them on 26). See the TxGemma paper for detailed results.TxGemma also includes 9B and 27B ‘chat’ versions. These models have general instruction tuning data added to their training, enabling them to explain their reasoning, answer complex questions, and engage in multi-turn discussions. For example, a researcher could ask TxGemma-Chat why it predicted a particular molecule to be toxic and receive an explanation based on the molecule’s structure. This conversational capability comes at a small cost to the raw performance on therapeutic tasks compared to TxGemma-Predict.As part of the release, we’re including a fine-tuning example Colab notebook that demonstrates how developers can adapt TxGemma to their own therapeutic data and tasks. This notebook uses the TrialBench dataset to show how to fine-tune TxGemma for predicting adverse events in clinical trials. Fine-tuning allows researchers to leverage their proprietary data to create models tailored to their unique research needs, possibly leading to even more accurate predictions that help researchers assess how safe or or effective a potential new therapy might be.Beyond single-step predictions, we’re demonstrating how TxGemma can be integrated into agentic systems to tackle more complex research problems. Standard language models often struggle with tasks requiring up-to-date external knowledge or multi-step reasoning. To address this, we’ve developed Agentic-Tx, a therapeutics-focused agentic system powered by Gemini 2.0 Pro. Agentic-Tx is equipped with 18 tools, including:Agentic-Tx achieves state-of-the-art results on reasoning-intensive chemistry and biology tasks from benchmarks including Humanity’s Last Exam and ChemBench. We are including a Colab notebook with our release to demonstrate how Agentic-Tx can be used to orchestrate complex workflows and answer multi-step research questions.Sorry, your browser doesn’t support playback for this videoYou can access TxGemma on both Vertex AI Model Garden and Hugging Face today. We encourage you to explore the models, try out the inference, fine-tuning, and agent Colab notebooks, and share your feedback! As an open model, TxGemma is designed to be further improved – researchers can fine-tune it with their data for specific therapeutic development use-cases. We’re excited to see how the community will use TxGemma to accelerate therapeutic discovery.Key contributors to this project include: Eric Wang, Samuel Schmidgall, Fan Zhang, Paul F. Jaeger, Rory Pilgrim and Tiffany Chen. We also thank Shravya Shetty, Dale Webster, Avinatan Hassidim, Yossi Matias, Yun Liu, Rachelle Sico, Phoebe Kirk, Fereshteh Mahvar, Can “John” Kirmizi, Fayaz Jamil, Tim Thelin, Glenn Cameron, Victor Cotruta, David Fleet, Jon Shlens, Omar Sanseviero, Joe Fernandez, and Joëlle Barral, for their feedback and support throughout this project.Simulating a neural operating system with Gemini 2.5 Flash-LiteIntroducing Gemma 3n: The developer guideUnlock deeper insights with the new Python client library for Data Commons</p>]]></content><author><name></name></author><summary type="html"><![CDATA[TxGemma is a collection of open models designed to improve efficiency of therapeutic development using language models.]]></summary></entry></feed>